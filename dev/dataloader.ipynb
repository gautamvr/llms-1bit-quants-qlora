{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbe0b3f-1d09-4925-b6d3-e392e656322b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Literal, Optional\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset       \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670a4f31-b22c-445c-92e0-900ab2232dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_LANGUAGE: Literal[\n",
    "    \"go\", \"java\", \"javascript\", \"php\", \"python\", \"ruby\"\n",
    "] = \"python\"\n",
    "\n",
    "HF_DATASET_NAME = \"code_search_net\"\n",
    "DEFAULT_TOKENIZER = \"microsoft/phi-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac45671-71d2-446c-9e77-e3ca7010953c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664cd325-4688-4785-bd78-1c7edbfd347e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_codesearchnet(\n",
    "    language: str = DEFAULT_LANGUAGE,\n",
    "    split: str = \"train\"\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Load the CodeSearchNet split for `language`.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ds = datasets.load_dataset(\n",
    "        HF_DATASET_NAME,\n",
    "        language,\n",
    "        split=split,\n",
    "        trust_remote_code=True)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "014852df-63d1-49ce-8ea9-c1779eada520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(\n",
    "    ds: Dataset,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    text_field: str = \"func_code_string\",\n",
    "    max_length: int = 512,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Tokenise `text_field` and keep only the tokens.\n",
    "    \"\"\"\n",
    "    def _tokenise(batch):\n",
    "        tokenized = tokenizer(\n",
    "            batch[text_field],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    tokenised = ds.map(\n",
    "        _tokenise,\n",
    "        batched=True,\n",
    "        remove_columns=ds.column_names)\n",
    "    \n",
    "    tokenised.set_format(type=\"torch\",columns=[\"input_ids\", \"attention_mask\", \"labels\"])    \n",
    "    return tokenised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2045b7d5-70c8-4909-96e5-f3146f1991d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_dataloader(\n",
    "    tokenised_ds: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = True,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Wrap the Hugging Face `Dataset` into a PyTorch `DataLoader`.\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        tokenised_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle and not tokenised_ds.streaming,\n",
    "        collate_fn=lambda x: {\n",
    "            k: torch.stack([d[k] for d in x]) for k in x[0]\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abafbb3-f2ad-4f20-be0e-2ec63304bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_dataloader(\n",
    "    language: str = DEFAULT_LANGUAGE,\n",
    "    split: str = \"train\",\n",
    "    cache_dir: str | None = None,\n",
    "    tokenizer_name: str = DEFAULT_TOKENIZER,\n",
    "    batch_size: int = 4,\n",
    "    streaming: bool = False,\n",
    ") -> Tuple[DataLoader, PreTrainedTokenizerBase]:\n",
    "    \"\"\"\n",
    "    End-to-end helper: load -> tokenise -> return DataLoader & tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True,\n",
    "    )\n",
    "\n",
    "    ds = load_codesearchnet(\n",
    "        language=language,\n",
    "        split=split,\n",
    "        cache_dir=cache_dir,\n",
    "        streaming=streaming,\n",
    "    )\n",
    "    tokenised = tokenize_dataset(ds, tokenizer)\n",
    "    loader = build_dataloader(tokenised, batch_size=batch_size)\n",
    "    return loader, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cce37b5-df76-4dfb-8257-8a430f01a277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = load_codesearchnet(DEFAULT_LANGUAGE, \"train\")\n",
    "test_data = load_codesearchnet(DEFAULT_LANGUAGE, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d867c1-8128-449c-8057-447c0eee7d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2543a72-44e3-4c0b-9224-51e0bc2a831c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7dd2e2-86a9-4eac-8971-29ac26914184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa5adba-b1a5-4682-b42a-384d3a78418a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████| 412178/412178 [17:04<00:00, 402.29 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████| 22176/22176 [00:56<00:00, 395.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenised_train_data = tokenize_dataset(train_data, tokenizer)\n",
    "tokenised_test_data = tokenize_dataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76507c97-b43f-4d65-99ba-93fd8411670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_48659/3348828132.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_baseline\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_train_data,\n",
    "    eval_dataset=tokenised_test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c2a2b8f-8dc4-4ba6-9652-e1e3533bd5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1408' max='103045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1408/103045 06:01 < 7:15:16, 3.89 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/transformers/trainer.py:2593\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2589\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2590\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2593\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2599\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2601\u001b[0m ):\n\u001b[1;32m   2602\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/accelerate/accelerator.py:2610\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2608\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[0;32m-> 2610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:95\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[1;32m     94\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m---> 95\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([norm\u001b[38;5;241m.\u001b[39mto(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n",
      "File \u001b[0;32m~/e6692-2025spring-finalproject-qbit-gv2359-rmm2292/venv/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[1;32m     94\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m---> 95\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06ce4d-f8e0-4da6-a667-18107c6fd3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qbit-venv)",
   "language": "python",
   "name": "qbit-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
